{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 103) (85, 103) (38, 103) (253,) (85,) (38,)\n",
      "\u001b[1m Results for RIDGE_AUC:\u001b[0m\n",
      " Returned hyperparameter: {'solver': 'liblinear'}\n",
      " Best classification score in validation set is: 79.5\n",
      "--------- 3.55 secondes ---------\n",
      "-----------------------------------------------------\n",
      "            Proba  Pred  Cap1  Cap2  Cap3  Qty1  Qty2  Qty3  Short\n",
      "30/04/2012      0     0   100   100   100     0     0     0  False\n",
      "[1]\n",
      "\u001b[1m Results for RIDGE_Class:\u001b[0m\n",
      " Returned hyperparameter: {'solver': 'lbfgs'}\n",
      " Best classification score in validation set is: 80.30000000000001\n",
      "--------- 0.13 secondes ---------\n",
      "-----------------------------------------------------\n",
      "            Proba  Pred  Cap1  Cap2  Cap3  Qty1  Qty2  Qty3  Short\n",
      "30/04/2012      0     0   100   100   100     0     0     0  False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ammyd\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ammyd\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1]\n",
      "\u001b[1m Results for RIDGE_Dev:\u001b[0m\n",
      " Returned hyperparameter: {'solver': 'lbfgs'}\n",
      " Best classification score in validation set is: 80.30000000000001\n",
      "--------- 0.15 secondes ---------\n",
      "-----------------------------------------------------\n",
      "            Proba  Pred  Cap1  Cap2  Cap3  Qty1  Qty2  Qty3  Short\n",
      "30/04/2012      0     0   100   100   100     0     0     0  False\n",
      "[1]\n",
      "\u001b[1m Results for LASSO_AUC:\u001b[0m\n",
      " Returned hyperparameter: {'solver': 'liblinear'}\n",
      " Best classification score in validation set is: 80.30000000000001\n",
      "--------- 0.06 secondes ---------\n",
      "-----------------------------------------------------\n",
      "            Proba  Pred  Cap1  Cap2  Cap3  Qty1  Qty2  Qty3  Short\n",
      "30/04/2012      0     0   100   100   100     0     0     0  False\n",
      "[1]\n",
      "[LightGBM] [Warning] boosting_type is set=gbdt, boosting_type= will be ignored. Current value: boosting_type=gbdt\n",
      "[LightGBM] [Warning] learning_rate is set=0.1, learning_rate= will be ignored. Current value: learning_rate=0.1\n",
      "[LightGBM] [Warning] Unknown parameter: gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: 0.01\n",
      "\u001b[1m Results for LIGHTGBM:\u001b[0m\n",
      " Returned hyperparameter: {'boosting_type ': 'gbdt', 'learning_rate ': 0.01, 'num_leaves': 100}\n",
      " Best classification score in validation set is: 77.60000000000001\n",
      "--------- 1.39 secondes ---------\n",
      "-----------------------------------------------------\n",
      "            Proba  Pred  Cap1  Cap2  Cap3  Qty1  Qty2  Qty3  Short\n",
      "30/04/2012      0     0   100   100   100     0     0     0  False\n",
      "[1]\n",
      "\u001b[1m Results for LASSO_Class:\u001b[0m\n",
      " Returned hyperparameter: {'solver': 'liblinear'}\n",
      " Best classification score in validation set is: 80.30000000000001\n",
      "--------- 0.07 secondes ---------\n",
      "-----------------------------------------------------\n",
      "            Proba  Pred  Cap1  Cap2  Cap3  Qty1  Qty2  Qty3  Short\n",
      "30/04/2012      0     0   100   100   100     0     0     0  False\n",
      "[1]\n",
      "\u001b[1m Results for LASSO_Dev:\u001b[0m\n",
      " Returned hyperparameter: {'solver': 'liblinear'}\n",
      " Best classification score in validation set is: 80.30000000000001\n",
      "--------- 0.05 secondes ---------\n",
      "-----------------------------------------------------\n",
      "            Proba  Pred  Cap1  Cap2  Cap3  Qty1  Qty2  Qty3  Short\n",
      "30/04/2012      0     0   100   100   100     0     0     0  False\n",
      "[1]\n",
      "\u001b[1m Results for Naive_B:\u001b[0m\n",
      " Returned hyperparameter: {'alpha': 0.2}\n",
      " Best classification score in validation set is: 78.4\n",
      "--------- 0.07 secondes ---------\n",
      "-----------------------------------------------------\n",
      "            Proba  Pred  Cap1  Cap2  Cap3  Qty1  Qty2  Qty3  Short\n",
      "30/04/2012      0     0   100   100   100     0     0     0  False\n",
      "[1]\n",
      "\u001b[1m Results for Bagging_Classfer:\u001b[0m\n",
      " Returned hyperparameter: {'n_estimators': 10}\n",
      " Best classification score in validation set is: 76.4\n",
      "--------- 2.05 secondes ---------\n",
      "-----------------------------------------------------\n",
      "            Proba  Pred  Cap1  Cap2  Cap3  Qty1  Qty2  Qty3  Short\n",
      "30/04/2012      0     0   100   100   100     0     0     0  False\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from  statsmodels.tsa.stattools import acf \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Scikit-learn\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV,RidgeClassifier,RidgeClassifierCV\n",
    "from sklearn.ensemble  import AdaBoostClassifier, GradientBoostingClassifier,RandomForestClassifier,BaggingClassifier \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB,CategoricalNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def strategy_long_only(Capital, Qty, Vix, Pred):\n",
    "    \n",
    "    if Qty != 0 and Pred == 0:\n",
    "        Capital = Qty * Vix\n",
    "        Qty = 0\n",
    "    if Qty == 0 and Pred == 1:\n",
    "        Qty = Capital / Vix\n",
    "        Capital = 0\n",
    "    return Capital,Qty\n",
    "\n",
    "def strat_long_short(Capital, Qty, Vix, Proba, Short):\n",
    "\n",
    "    if proba>0.55 and Capital!= 0 :\n",
    "        if Short == True:\n",
    "            Capital -= Qty * Vix\n",
    "        Qty = Capital/Vix\n",
    "        Capital = 0\n",
    "        Short = False\n",
    "    \n",
    "    elif proba< 0.45 and Short== False:\n",
    "        if Qty !=0:\n",
    "            Capital = Qty*Vix\n",
    "\n",
    "        Qty = Capital / Vix\n",
    "        Capital += Qty*Vix\n",
    "        Short = True\n",
    "    return Capital,Qty, Short\n",
    "\n",
    "def strategy_long_only_SP500(Capital, Qty, SP500, Proba):\n",
    "    \n",
    "    if Qty != 0 and Proba > 0.55:\n",
    "        Capital = Qty * SP500\n",
    "        Qty = 0\n",
    "    if Qty == 0 and Proba < 0.45:\n",
    "        Qty = Capital / SP500\n",
    "        \n",
    "    return Capital,Qty\n",
    "\n",
    "data = pd.read_csv(r\"df_merged_data_hurst.csv\",sep = ';', decimal = ',')\n",
    "\n",
    "sp500 = data[[\"Date\",\"SP500\"]].copy()\n",
    "\n",
    "data[\"SP500\"] = data[\"SP500\"].pct_change()\n",
    "\n",
    "data = data.set_index(\"Date\")\n",
    "\n",
    "list_name_t1 = [i+\"_t_1\" for i in data.columns]\n",
    "list_name_t2 = [i+\"_t_2\" for i in data.columns]\n",
    "\n",
    "df_t1 = data.shift()\n",
    "df_t2 = data.shift(2)\n",
    "data[\"VIX_t_3\"] = data[\"VIX\"].shift(3)\n",
    "data[\"10Y\"] = data[\"10Y\"].diff()\n",
    "data[\"3M\"] = data[\"3M\"].diff()\n",
    "\n",
    "df_t1.columns = list_name_t1 \n",
    "df_t2.columns = list_name_t2\n",
    "\n",
    "\n",
    "n_data = pd.concat([data,df_t1,df_t2], axis = 1 ).dropna()\n",
    "\n",
    "n_data[\"Y_pred\"]     = 1*(n_data[\"VIX\"] > n_data[\"VIX_t_1\"]) \n",
    "n_data[\"Y_t_1_pred\"] = 1*(n_data[\"VIX_t_1\"] > n_data[\"VIX_t_2\"]) \n",
    "n_data[\"Y_t_2_pred\"] = 1*(n_data[\"VIX_t_2\"] > n_data[\"VIX_t_3\"]) \n",
    "\n",
    "del n_data[\"VIX_t_3\"]\n",
    "\n",
    "#n_data.to_csv(\"Dataset.csv\")\n",
    "\n",
    "X_all = n_data.loc[:, ~n_data.columns.isin(['Y_pred',\"VIX\"])]\n",
    "\n",
    "Y_all = n_data[\"Y_pred\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, Y_all, test_size=0.1, random_state=1, shuffle = False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1, shuffle = False) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "print(X_train.shape,  X_val.shape,X_test.shape, y_train.shape, y_val.shape,y_test.shape)\n",
    "\n",
    "models = [LogisticRegression,LinearDiscriminantAnalysis,BernoulliNB, BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,\n",
    "KNeighborsClassifier] #pas de rpart ni bglm car pas de package sous Python (uniquement sous R, et pas le temps de recoder)\n",
    "\n",
    "#          ('EN_AUC',LogisticRegression(penalty='elasticnet',solver='saga')),\n",
    "#           ('EN_Class',LogisticRegression(penalty='elasticnet',solver='saga')),\n",
    "#           ('EN_Dev',LogisticRegression(penalty='elasticnet',solver='saga')),\n",
    "#           ('LDA',LinearDiscriminantAnalysis()),\n",
    "#           ('RDA',LinearDiscriminantAnalysis(solver='eigen',shrinkage='auto')),\n",
    "#Random Forest,B_GLM\n",
    "models = [\n",
    "    ('RIDGE_AUC',LogisticRegression()),\n",
    "          ('RIDGE_Class',LogisticRegression()),\n",
    "          ('RIDGE_Dev',LogisticRegression()),\n",
    "       ('LASSO_AUC',LogisticRegression(penalty='l1')),\n",
    "    (\"LIGHTGBM\",LGBMClassifier()),\n",
    "          ('LASSO_Class',LogisticRegression(penalty='l1')),\n",
    "          ('LASSO_Dev',LogisticRegression(penalty='l1')),\n",
    "          ('Naive_B',BernoulliNB()),\n",
    "          ('Bagging_Classfer',BaggingClassifier()),\n",
    "          ('Random_Frst',RandomForestClassifier()),\n",
    "          ('adaboost',AdaBoostClassifier(base_estimator=DecisionTreeClassifier())),\n",
    "          ('GradientB_class',GradientBoostingClassifier()),\n",
    "          ('KNearest_Neigh',KNeighborsClassifier()),\n",
    "          (\"xgboost\",  XGBClassifier(min_child_weight=0,\\\n",
    "                       gamma=0, subsample=0.7,\\\n",
    "                       colsample_bytree=1.0,\\\n",
    "                       objective='reg:squarederror', nthread=-1,\\\n",
    "                       scale_pos_weight=1, seed=27,\\\n",
    "                       reg_alpha=0.00006))\n",
    "         ]\n",
    "\n",
    "grids = {\n",
    "    'RIDGE_AUC':{'solver':['newton-cg','lbfgs','liblinear','sag','saga']},\n",
    "         'RIDGE_Class':{'solver':['newton-cg','lbfgs','liblinear','sag','saga']},\n",
    "         'RIDGE_Dev':{'solver':['newton-cg','lbfgs','liblinear','sag','saga']},\n",
    "        'LASSO_AUC':{'solver':['liblinear','saga']},\n",
    "            \"LIGHTGBM\":{\"boosting_type \" : [\"gbdt\",\"dart\"],\n",
    "                       \"num_leaves\": [100,150],\n",
    "                        \"learning_rate \" : [0.01,0.2,0.3]\n",
    "                       },\n",
    "         'LASSO_Class':{'solver':['liblinear','saga']},\n",
    "         'LASSO_Dev':{'solver':['liblinear','saga']},\n",
    "         'Naive_B':{'alpha':[0.2,0.4,0.6,0.8]},\n",
    "         'Bagging_Classfer':{'n_estimators':[10,100,1000]},\n",
    "         'Random_Frst':{'n_estimators':[5,20,50,100],\n",
    "                         'max_depth':[5,7,10,12,15],\n",
    "                         'min_samples_split':[2,4,6],\n",
    "                         'min_samples_leaf':[1,2,3,4,5],\n",
    "                         'max_features':['sqrt', 'log2'],\n",
    "                        },\n",
    "         'adaboost' : {'n_estimators': [100,300,500],\n",
    "               'learning_rate' :[0.1,0.5,1.0],\n",
    "               'base_estimator__max_depth':[i for i in range(2,7,2)], #Pour atteindre les paramètres du WL\n",
    "               'base_estimator__min_samples_leaf' :[0.01]},\n",
    "         'KNearest_Neigh':{'leaf_size':list(range(1,50)),\n",
    "                           'n_neighbors':list(range(1,30)),\n",
    "                           'p':[1,2]},\n",
    "         'GradientB_class':{'learning_rate': [0.01,0.05,0.1],\n",
    "                  'subsample'    : [1.0,0.9, 0.8],\n",
    "                  'n_estimators' : [300,500],      \n",
    "                  'max_depth'    : [3,5]},\n",
    "         'xgboost' : { 'max_depth': [2,4, 6],\n",
    "                        'n_estimators': [1000, 3000, 5000],\n",
    "                        'learning_rate': [0.09, 0.1,0.11]}\n",
    "        }\n",
    "\n",
    "sp500.set_index(\"Date\",inplace=True)\n",
    "\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def calculate_dev(y_true,y_pred):\n",
    "    return (2*(y_true * np.log(y_true/y_pred) - (y_true-y_pred))).sum()\n",
    "\n",
    "columns_dic = [\"Proba\",\"Pred\",\"Cap1\",\"Cap2\",\"Cap3\",\"Qty1\",\"Qty2\",\"Qty3\",\"Short\"]\n",
    "modelOpt_list = []\n",
    "\n",
    "dico_model = {}\n",
    "\n",
    "dat = X_train.iloc[-1,:].name\n",
    "dict_s = {\"Proba\" : 0 ,\"Pred\" :0,\"Cap1\" : 100,\"Cap2\": 100,\"Cap3\":100,\"Qty1\":0,\"Qty2\":0,\"Qty3\":0, \"Short\" : False}\n",
    "\n",
    "for name, model in models:\n",
    "    dico_model[name] = pd.DataFrame(index = [dat],data = dict_s)\n",
    "for i,p in enumerate(range(len(X_val))):\n",
    "    for name, model in models:\n",
    "        \n",
    "        #Vix = data.iloc[p,1]\n",
    "        start_time=time.time()\n",
    "        #pipe = Pipeline(steps=[('preprocessor', preproc), (name, model)])\n",
    "          \n",
    "        \n",
    "        # Elastic Net \n",
    "        \n",
    "        regr = ElasticNet(random_state=0,alpha = 0.50)\n",
    "        \n",
    "        regr.fit(X_train, y_train)\n",
    "\n",
    "        #print(\"Elastic Net picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "        coef = pd.Series(regr.coef_, index = X_train.columns)\n",
    "\n",
    "        X_train_temp = X_train[coef[coef!=0].index.to_list()]\n",
    "        \n",
    "        if \"AUC\" in name : \n",
    "            clf = GridSearchCV(model,grids[name],scoring='roc_auc', cv=3,n_jobs=-1)\n",
    "        if \"Class\" in name : \n",
    "            clf = GridSearchCV(model, grids[name],scoring='CLASS', cv=3,n_jobs=-1)\n",
    "        if \"Dev\" in name : \n",
    "            clf = GridSearchCV(model, grids[name],scoring='DEV', cv=3,n_jobs=-1)\n",
    "            \n",
    "        clf = GridSearchCV( model,grids[name],scoring=\"accuracy\", cv=3,n_jobs=-1)\n",
    "        clf.fit(X_train_temp, y_train)\n",
    "        modelOpt_list.append((name,clf))\n",
    "        \n",
    "        \n",
    "        print('\\033[1m' + \" Results for {}:\".format(name) + '\\033[0m')\n",
    "        print(' Returned hyperparameter: {}'.format(clf.best_params_))\n",
    "        print(' Best classification score in validation set is: {}'.format(round(clf.best_score_,3)*100))\n",
    "        #print(' classification score on test is: {}'.format(- clf.score(X_test, y_test)*100),end='\\n\\n')\n",
    "        print(\"--------- %s secondes ---------\" % (round(time.time() - start_time,2)))\n",
    "        print('-----------------------------------------------------')\n",
    "        \n",
    "\n",
    "        \n",
    "        X_val_temp = X_val[coef[coef!=0].index.to_list()]\n",
    "        # Prédiction sur une seule observation\n",
    "        pred  = clf.predict(pd.DataFrame(X_val_temp.iloc[i,:]).T)\n",
    "        proba = clf.predict_proba(pd.DataFrame(X_val_temp.iloc[i,:]).T)[0][1] \n",
    "        date = X_val_temp.index[i]\n",
    "        \n",
    "        print(dico_model[name])\n",
    "       \n",
    "        # Stratégie\n",
    "        print(pred)\n",
    "        Cap1,Qty1 = strategy_long_only(dico_model[name][\"Cap1\"].values[i], dico_model[name][\"Qty1\"].values[i], data[\"VIX\"].loc[date], pred)\n",
    "        Cap2,Qty2,Short = strat_long_short(dico_model[name][\"Cap2\"].values[i], dico_model[name][\"Qty2\"].values[i], data[\"VIX\"].loc[date], proba, dico_model[name][\"Short\"].values[i])\n",
    "        Cap3,Qty3 = strategy_long_only_SP500(dico_model[name][\"Cap3\"].values[i], dico_model[name][\"Qty3\"].values[i], sp500.loc[date].values[0], proba)\n",
    "        \n",
    "        \n",
    "        dict_s = {\"Proba\" : proba ,\"Pred\" :pred,\"Cap1\" : Cap1,\"Cap2\": Cap2,\"Cap3\":Cap3,\"Qty1\":Qty1,\"Qty2\":Qty2,\"Qty3\":Qty3, \"Short\" : Short}\n",
    "        \n",
    "        dico_model[name] = dico_model[name].append(pd.DataFrame(dict_s,index=[X_val.index[i]]))\n",
    "        \n",
    "        # Append une observation au Train \n",
    "        X_train.loc[date] = X_val.iloc[i,]\n",
    "        y_train.loc[date] = y_train.iloc[i]\n",
    "        \n",
    "\n",
    "for model in dico_model.keys():\n",
    "    print(dico_model[model])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_model[\"LIGHTGBM\"].to_csv(\"LIGHTGBM_ACCuracy.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
